{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1515aa05",
   "metadata": {},
   "source": [
    "# K-Means Clustering Analysis\n",
    "\n",
    "This notebook implements the clustering component of the project.\n",
    "We load the feature matrix, preprocess it, run K-Means with several values of *k*,\n",
    "evaluate the clustering using different metrics, select the best configuration,\n",
    "and visualize the resulting clusters using PCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8be0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9807a9af",
   "metadata": {},
   "source": [
    "## 1. Load & Preprocess Data\n",
    "\n",
    "We load the `.npy` feature matrix, standardize all features using `StandardScaler`,\n",
    "and keep the full dataset for clustering. Standardization is important because\n",
    "K-Means is distance-based and is sensitive to the scale of the features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ce24c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feature matrix (ensure X_features.npy is in the same folder as this notebook)\n",
    "X = np.load('X_features.npy')\n",
    "print('Original shape:', X.shape)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print('Scaled shape:', X_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e45d218",
   "metadata": {},
   "source": [
    "## 2. K-Means Experiments\n",
    "\n",
    "We test K-Means with different numbers of clusters:\n",
    "`k = 10, 20, 30, 40, 50`.\n",
    "\n",
    "For each configuration we compute three clustering quality metrics:\n",
    "- **Silhouette Score** (higher is better)\n",
    "- **Calinski–Harabasz Index** (higher is better)\n",
    "- **Davies–Bouldin Index** (lower is better)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dbf7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [10, 20, 30, 40, 50]\n",
    "results = []\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "    sil = silhouette_score(X_scaled, labels)\n",
    "    ch = calinski_harabasz_score(X_scaled, labels)\n",
    "    db = davies_bouldin_score(X_scaled, labels)\n",
    "\n",
    "    results.append([k, sil, ch, db])\n",
    "    print(f'k={k} | Silhouette={sil:.4f} | CH={ch:.2f} | DB={db:.4f}')\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['k', 'silhouette', 'calinski_harabasz', 'davies_bouldin'])\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3eb74b",
   "metadata": {},
   "source": [
    "## 3. Compare Clustering Metrics\n",
    "\n",
    "We now visualize how each metric behaves as a function of the number of clusters.\n",
    "This helps to see trends and to justify the chosen value of *k*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd03d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(results_df['k'], results_df['silhouette'], marker='o')\n",
    "plt.xlabel('k (number of clusters)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score vs k')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(results_df['k'], results_df['calinski_harabasz'], marker='o')\n",
    "plt.xlabel('k (number of clusters)')\n",
    "plt.ylabel('Calinski–Harabasz Index')\n",
    "plt.title('Calinski–Harabasz Index vs k')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(results_df['k'], results_df['davies_bouldin'], marker='o')\n",
    "plt.xlabel('k (number of clusters)')\n",
    "plt.ylabel('Davies–Bouldin Index')\n",
    "plt.title('Davies–Bouldin Index vs k')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6dd847",
   "metadata": {},
   "source": [
    "## 4. Select Best k\n",
    "\n",
    "We select the best number of clusters using the **Silhouette Score** as the\n",
    "primary criterion (higher is better). The other two metrics are used as\n",
    "secondary checks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7edb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_row = results_df.iloc[results_df['silhouette'].idxmax()]\n",
    "best_k = int(best_row['k'])\n",
    "print('Best k (by silhouette):', best_k)\n",
    "best_row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7764df51",
   "metadata": {},
   "source": [
    "## 5. Run Final K-Means Model and Save Labels\n",
    "\n",
    "We now fit K-Means again using the selected number of clusters `best_k`.\n",
    "We store the labels in a separate `.npy` file so they can be reused by\n",
    "other parts of the project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f77ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "best_labels = best_kmeans.fit_predict(X_scaled)\n",
    "\n",
    "np.save('best_kmeans_labels.npy', best_labels)\n",
    "print('Saved labels to best_kmeans_labels.npy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab92790d",
   "metadata": {},
   "source": [
    "## 6. PCA Visualization of Clusters\n",
    "\n",
    "We reduce the standardized data to 2 dimensions using PCA and plot the\n",
    "samples colored by their cluster assignment. This gives an intuitive view of\n",
    "how the clusters are separated in a low-dimensional space.\n",
    "\n",
    "Figures are saved in the `images/` folder for inclusion in the report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c89b672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure images folder exists\n",
    "os.makedirs('images', exist_ok=True)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=best_labels, s=10, cmap='tab20')\n",
    "plt.title(f'PCA Visualization - K-Means (k={best_k})')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/pca_clusters.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f69e09c",
   "metadata": {},
   "source": [
    "## 7. Cluster Size Histogram\n",
    "\n",
    "We also look at the distribution of samples across clusters. This helps us\n",
    "understand whether some clusters are very small (potentially noise) or if the\n",
    "clusters are relatively balanced.\n",
    "\n",
    "This figure is also saved to the `images/` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7b312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x=best_labels)\n",
    "plt.title(f'Cluster Size Distribution (k={best_k})')\n",
    "plt.xlabel('Cluster Label')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/cluster_histogram.png', dpi=300)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
